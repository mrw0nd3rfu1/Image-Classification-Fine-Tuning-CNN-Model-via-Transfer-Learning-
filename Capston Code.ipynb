{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fine_tune our data first to accomplish that we will use fine_tune.py\n",
    "It needs keras library which anaconda already have but can be downloaded from keras if doesn't.\n",
    "This class trains the model which can be implemented as:\n",
    "\n",
    "python code/fine_tune.py <data_dir/> <model_dir/>\n",
    "\n",
    "The data and model can be find on the dataset link provided early:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e0f9027e2759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.applications import *\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as k\n",
    "\n",
    "# fix seed for reproducible results (only works on CPU, not GPU)\n",
    "seed = 9\n",
    "np.random.seed(seed=seed)\n",
    "tf.set_random_seed(seed=seed)\n",
    "\n",
    "# hyper parameters for model\n",
    "nb_classes = 2  # number of classes\n",
    "based_model_last_block_layer_number = 126 \n",
    "img_width, img_height = 299, 299\n",
    "batch_size = 32  \n",
    "nb_epoch = 50 \n",
    "learn_rate = 1e-4  \n",
    "momentum = .9 \n",
    "transformation_ratio = .05  # how aggressive will be the data augmentation/transformation\n",
    "\n",
    "\n",
    "def train(train_data_dir, validation_data_dir, model_path):\n",
    "    # Pre-Trained CNN Model using imagenet dataset for pre-trained weights\n",
    "    base_model = Xception(input_shape=(img_width, img_height, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # Top Model Block\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "    # add your top layer block to your base model\n",
    "    model = Model(base_model.input, predictions)\n",
    "    print(model.summary())\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all layers of the based model that is already pre-trained.\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Read Data and Augment it: Make sure to select augmentations that are appropriate to your images.\n",
    "    # To save augmentations un-comment save lines and add to your flow parameters.\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                       rotation_range=transformation_ratio,\n",
    "                                       shear_range=transformation_ratio,\n",
    "                                       zoom_range=transformation_ratio,\n",
    "                                       cval=transformation_ratio,\n",
    "                                       horizontal_flip=True,\n",
    "                                       vertical_flip=True)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    os.makedirs(os.path.join(os.path.abspath(train_data_dir), '../preview'), exist_ok=True)\n",
    "    train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                        target_size=(img_width, img_height),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')\n",
    "    # save_to_dir=os.path.join(os.path.abspath(train_data_dir), '../preview')\n",
    "    # save_prefix='aug',\n",
    "    # save_format='jpeg')\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                                  target_size=(img_width, img_height),\n",
    "                                                                  batch_size=batch_size,\n",
    "                                                                  class_mode='categorical')\n",
    "\n",
    "    model.compile(optimizer='nadam',\n",
    "                  loss='categorical_crossentropy',  # categorical_crossentropy if multi-class classifier\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # save weights of best training epoch: monitor either val_loss or val_acc\n",
    "\n",
    "    top_weights_path = os.path.join(os.path.abspath(model_path), 'top_model_weights.h5')\n",
    "    callbacks_list = [\n",
    "        ModelCheckpoint(top_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "        EarlyStopping(monitor='val_acc', patience=5, verbose=0)\n",
    "    ]\n",
    "\n",
    "    # Train Simple CNN\n",
    "    model.fit_generator(train_generator,\n",
    "                        samples_per_epoch=train_generator.nb_sample,\n",
    "                        nb_epoch=nb_epoch / 5,\n",
    "                        validation_data=validation_generator,\n",
    "                        nb_val_samples=validation_generator.nb_sample,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "    # verbose\n",
    "    print(\"\\nStarting to Fine Tune Model\\n\")\n",
    "\n",
    "    # we re-load model weights to ensure the best epoch is selected and not the last one.\n",
    "    model.load_weights(top_weights_path)\n",
    "\n",
    "    # based_model_last_block_layer_number points to the layer in your model you want to train.\n",
    "    # layers before this number will used the pre-trained weights, layers above and including this number\n",
    "    # will be re-trained based on the new data.\n",
    "    for layer in model.layers[:based_model_last_block_layer_number]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[based_model_last_block_layer_number:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # compile the model with a SGD/momentum optimizer\n",
    "    # and a very slow learning rate.\n",
    "    model.compile(optimizer='nadam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # save weights of best training epoch: monitor either val_loss or val_acc\n",
    "    final_weights_path = os.path.join(os.path.abspath(model_path), 'model_weights.h5')\n",
    "    callbacks_list = [\n",
    "        ModelCheckpoint(final_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "        EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "    ]\n",
    "\n",
    "    # fine-tune the model\n",
    "    model.fit_generator(train_generator,\n",
    "                        samples_per_epoch=train_generator.nb_sample,\n",
    "                        nb_epoch=nb_epoch,\n",
    "                        validation_data=validation_generator,\n",
    "                        nb_val_samples=validation_generator.nb_sample,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "    # save model\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(os.path.abspath(model_path), 'model.json'), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not len(sys.argv) == 3:\n",
    "        print('Arguments must match:\\npython code/fine_tune.py <data_dir/> <model_dir/>')\n",
    "        print('Example: python code/fine_tune.py data/dogs_cats/ model/dog_cats/')\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        data_dir = os.path.abspath(sys.argv[1])\n",
    "        train_dir = os.path.join(os.path.abspath(data_dir), 'train')  # Inside, each class should have it's own folder\n",
    "        validation_dir = os.path.join(os.path.abspath(data_dir), 'validation')  # each class should have it's own folder\n",
    "        model_dir = os.path.abspath(sys.argv[2])\n",
    "\n",
    "        os.makedirs(os.path.join(os.path.abspath(data_dir), 'preview'), exist_ok=True)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    train(train_dir, validation_dir, model_dir)  # train model\n",
    "\n",
    "    # release memory\n",
    "    k.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code we can classify the data.\n",
    "it can be done from following:\n",
    "python code/classify.py <model_dir/> <test_dir/> <results_dir/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhinav\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8adb08093c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprobas_to_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils.np_utils import probas_to_classes\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as k\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# fix seed for reproducible results (only works on CPU, not GPU)\n",
    "seed = 9\n",
    "np.random.seed(seed=seed)\n",
    "tf.set_random_seed(seed=seed)\n",
    "\n",
    "img_width, img_height = 299, 299  \n",
    "batch_size = 32  \n",
    "\n",
    "# default paths\n",
    "model_name = 'model.json'\n",
    "model_weights = 'top_model_weights.h5'\n",
    "results_name = 'predictions.csv'\n",
    "\n",
    "\n",
    "def classify(trained_model_dir, test_data_dir, results_path):\n",
    "    # load json and create model\n",
    "    json_file = open(os.path.join(trained_model_dir, model_name), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(os.path.join(trained_model_dir, model_weights))\n",
    "\n",
    "    # Read Data\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                                      target_size=(img_width, img_height),\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=False)\n",
    "\n",
    "    # Calculate class posteriors probabilities\n",
    "    y_probabilities = model.predict_generator(test_generator,\n",
    "                                              val_samples=test_generator.nb_sample)\n",
    "    # Calculate class labels\n",
    "    y_classes = probas_to_classes(y_probabilities)\n",
    "    filenames = [filename.split('/')[1] for filename in test_generator.filenames]\n",
    "    ids = [filename.split('.')[0] for filename in filenames]\n",
    "\n",
    "    # save results as a csv file in the specified results directory\n",
    "    with open(os.path.join(results_path, results_name), 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(('id', 'class0_prob', 'class1_prob', 'label'))\n",
    "        writer.writerows(zip(ids, y_probabilities[:, 0], y_probabilities[:, 1], y_classes))\n",
    "\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not len(sys.argv) == 4:\n",
    "        print('Arguments must match:\\npython code/classify.py <model_dir/> <test_dir/> <results_dir/>')\n",
    "        print('Example: python code/classify.py model/dogs_cats data/dogs_cats/test/ results/dogs_cats/')\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        model_dir = os.path.abspath(sys.argv[1])\n",
    "        test_dir = os.path.abspath(sys.argv[2])\n",
    "        results_dir = os.path.abspath(sys.argv[3])\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    classify(model_dir, test_dir, results_dir)  # train model\n",
    "\n",
    "    # release memory\n",
    "    k.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
